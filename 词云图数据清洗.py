import jieba
from collections import Counter
import pandas as pd
import re
import string
import streamlit as st
from streamlit_echarts import st_echarts
from streamlit.logger import get_logger

# jiebaç”¨äºä¸­æ–‡åˆ†è¯ï¼ŒCounterç”¨äºç»Ÿè®¡è¯é¢‘ï¼Œpandasç”¨äºæ•°æ®å¤„ç†ï¼Œreç”¨äºæ­£åˆ™è¡¨è¾¾å¼åŒ¹é…ï¼Œstringæä¾›å­—ç¬¦ä¸²æ“ä½œ

# è‡ªå®šä¹‰è¯å…¸ï¼Œæ·»åŠ ä¸åº”è¯¥è¢«åˆ†å¼€çš„è¯æ±‡ï¼Œå®šä¹‰ä¸€ä¸ªåˆ—è¡¨custom_wordsï¼ŒåŒ…å«ä¸åº”è¯¥è¢«åˆ†å¼€çš„è¯æ±‡ã€‚
# é€šè¿‡jieba.add_wordå‡½æ•°å°†è¿™äº›è¯æ±‡æ·»åŠ åˆ°jiebaçš„åˆ†è¯è¯å…¸ä¸­ï¼Œç¡®ä¿åˆ†è¯æ—¶è¿™äº›è¯æ±‡ä¸ä¼šè¢«é”™è¯¯åœ°åˆ‡åˆ†
custom_words = ["å¯¹æ¯”ç ”ç©¶", "éè¨€è¯­", "æ•™å­¦ç ”ç©¶", "å½±å“ç ”ç©¶", "å¤šæ¨¡æ€ç ”ç©¶", "è¡Œä¸ºç ”ç©¶", "ç¼–ç ä¸åˆ†æ", "è¯„ä»·ç ”ç©¶", "å½±å“",
                "åº”ç”¨è°ƒæŸ¥", "è°ƒæŸ¥ç ”ç©¶", "æ¡ˆä¾‹ç ”ç©¶", "ä½œç”¨"]
for word in custom_words:
    jieba.add_word(word)

# å®šä¹‰ä¸€ä¸ªå‡½æ•°load_stop_wordsï¼Œç”¨äºä»æ–‡ä»¶stop_words.txtä¸­åŠ è½½åœç”¨è¯åˆ—è¡¨ã€‚
def load_stop_words():
    stop_words = []
    with open('stop_words.txt', 'r', encoding='utf-8') as f:
        for line in f:
            stop_words.append(line.strip())
    return stop_words

stopwords = load_stop_words()

# withè¯­å¥æ‰“å¼€æŒ‡å®šè·¯å¾„ä¸‹çš„æ–‡æœ¬æ–‡ä»¶ï¼Œå¹¶è¯»å–å…¶å†…å®¹ï¼Œå­˜å‚¨åœ¨å˜é‡contentä¸­ä½¿ç”¨åŸå§‹å­—ç¬¦ä¸²æŒ‡å®šæ–‡ä»¶è·¯å¾„ï¼Œä»¥é¿å…è½¬ä¹‰å­—ç¬¦çš„é—®é¢˜
with open(r'C:\Users\è¶Šå¥½\Downloads\CNKI-20240509140652944.txt', 'r', encoding='utf-8') as f:
    content = f.read()

# ä½¿ç”¨re.subå‡½æ•°å’Œæ­£åˆ™è¡¨è¾¾å¼å»é™¤æ–‡æœ¬ä¸­çš„HTMLæ ‡ç­¾ã€‚
# æ­£åˆ™è¡¨è¾¾å¼<[^<]+?>åŒ¹é…HTMLæ ‡ç­¾ï¼Œre.subå°†è¿™äº›åŒ¹é…åˆ°çš„å†…å®¹æ›¿æ¢ä¸ºç©ºå­—ç¬¦ä¸²ï¼Œå³ç§»é™¤å®ƒä»¬
content = re.sub('<[^<]+?>', '', content)

# å®šä¹‰ä¸€ä¸ªå‡½æ•°remove_stopwordsï¼Œç”¨äºä»æ–‡æœ¬ä¸­å»é™¤åœç”¨è¯ã€‚
# è°ƒç”¨remove_stopwordså‡½æ•°ï¼Œå¹¶å°†å¤„ç†åçš„æ–‡æœ¬å­˜å‚¨åœ¨cleaned_text
def remove_stopwords(text, stopwords):
    words = text.split()
    # ç”¨äºåˆ›å»ºä¸€ä¸ªæ–°çš„åˆ—è¡¨cleaned_wordsï¼Œå…¶ä¸­åŒ…å«åŸå§‹åˆ—è¡¨wordsä¸­æ‰€æœ‰ä¸åœ¨åœç”¨è¯åˆ—è¡¨stopwordsä¸­çš„å•è¯ã€‚
    cleaned_words = [word for word in words if word.lower() not in stopwords]
    return ' '.join(cleaned_words)
cleaned_text = remove_stopwords(content, stopwords)
print("After removing stopwords:", cleaned_text)

# ä½¿ç”¨jiebaè¿›è¡Œåˆ†è¯ï¼Œä½¿ç”¨jieba.cutå‡½æ•°å¯¹å»é™¤åœç”¨è¯åçš„æ–‡æœ¬è¿›è¡Œåˆ†è¯ã€‚
# cut_all=Falseå‚æ•°è¡¨ç¤ºä½¿ç”¨ç²¾ç¡®æ¨¡å¼è¿›è¡Œåˆ†è¯ã€‚å°†åˆ†è¯ç»“æœè½¬æ¢ä¸ºåˆ—è¡¨ï¼Œå¹¶å­˜å‚¨åœ¨word_listä¸­
words = jieba.cut(cleaned_text, cut_all=False)
word_list = list(words)

# å†æ¬¡è¿‡æ»¤word_listä¸­çš„åœç”¨è¯ï¼Œç¡®ä¿æœ€ç»ˆçš„è¯åˆ—è¡¨ä¸­ä¸åŒ…å«åœç”¨è¯
word_list = [word for word in word_list if word not in stopwords]

# ç»Ÿè®¡è¯é¢‘
word_counts = Counter(word_list)

# åˆ›å»ºä¸€ä¸ªDataFrameæ¥å­˜å‚¨è¯é¢‘ï¼Œcolumns=['Word', 'Frequency']å®šä¹‰äº†DataFrameçš„åˆ—å
df = pd.DataFrame(list(word_counts.items()), columns=['Word', 'Frequency'])

# å¯¹DataFrameæŒ‰ç…§â€™Frequencyâ€™åˆ—è¿›è¡Œé™åºæ’åº
df = df.sort_values(by='Frequency', ascending=False)

# å°†DataFrameä¿å­˜åˆ°CSVæ–‡ä»¶
# UTF-8å’ŒUTF-8-SIGçš„ä¸»è¦åŒºåˆ«åœ¨äºå¯¹BOMçš„å¤„ç†æ–¹å¼ä¸åŒã€‚ åœ¨å¤§å¤šæ•°æƒ…å†µä¸‹ï¼Œæ¨èä½¿ç”¨UTF-8-SIGæ¥ç¼–ç å’Œä¿å­˜æ–‡æœ¬æ–‡ä»¶ï¼Œå› ä¸ºå®ƒå¯ä»¥æ›´å¥½åœ°å¤„ç†BOMé—®é¢˜ï¼Œæé«˜æ–‡ä»¶çš„å…¼å®¹æ€§å’Œå¯è¯»æ€§ã€‚
df.to_csv('word_frequencies.csv', index=False, encoding='utf-8-sig')


def run():
    st.set_page_config(
        page_title="Hello",
        page_icon="ğŸ‘‹",
    )

    st.write("# Welcome to Streamlit! ğŸ‘‹")

    url = st.text_input('Enter URL:')

    if url:
        r = requests.get(url)
        r.encoding = 'utf-8'
        text = r.text
        text = extract_body_text(text)

        text = remove_html_tags(text)
        text = remove_punctuation(text)
        # words = tokenize(text)
        # # words = stem_words(words)
        # words = remove_stopwords(text.split())

        # # è¯†åˆ«ä¸­è‹±æ–‡ï¼Œåˆ¤æ–­åˆ†è¯æ–¹å¼
        # import chardet
        # encoding = chardet.detect(text)['encoding']
        # if encoding == 'utf-8':
        #     words = jieba.cut(text)
        # else:
        #     words = text.split()

        # words = text.split()
        text = clean_text(text)
        words = segment(text)
        word_counts = Counter(words)

        top_words = word_counts.most_common(20)

        wordcloud_options = {
            "tooltip": {
                "trigger": 'item',
                "formatter": '{b} : {c}'
            },
            "xAxis": [{
                "type": "category",
                "data": [word for word, count in top_words],
                "axisLabel": {
                    "interval": 0,
                    "rotate": 30
                }
            }],
            "yAxis": [{"type": "value"}],
            "series": [{
                "type": "bar",
                "data": [count for word, count in top_words]
            }]
        }

        st_echarts(wordcloud_options, height='500px')

    # read_txtfile()

if __name__ == "__main__":
    run()